<!-- build time:Fri Feb 23 2018 18:32:34 GMT+0800 (CST) --><!doctype html><html class="theme-next mist use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css"><meta name="keywords" content="Keras,DQN,"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1"><meta name="description" content="基于Keras的200 行 Python 代码实现 DQN 玩 FlappyBird"><meta name="keywords" content="Keras,DQN"><meta property="og:type" content="article"><meta property="og:title" content="200行Keras代码实现DQN玩转FlappyBird"><meta property="og:url" content="https://ricky.moe/2016/07/19/Keras-DQN-Flappybird/index.html"><meta property="og:site_name" content="知识沉言"><meta property="og:description" content="基于Keras的200 行 Python 代码实现 DQN 玩 FlappyBird"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://7xwkbq.com1.z0.glb.clouddn.com/flappybird.gif"><meta property="og:image" content="https://ricky.moe/2016/07/19/Keras-DQN-Flappybird/flappybird2.jpg"><meta property="og:image" content="https://ricky.moe/2016/07/19/Keras-DQN-Flappybird/convolution_schematic.gif"><meta property="og:image" content="https://ricky.moe/2016/07/19/Keras-DQN-Flappybird/edge-detect.jpg"><meta property="og:updated_time" content="2017-10-29T10:16:27.891Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="200行Keras代码实现DQN玩转FlappyBird"><meta name="twitter:description" content="基于Keras的200 行 Python 代码实现 DQN 玩 FlappyBird"><meta name="twitter:image" content="http://7xwkbq.com1.z0.glb.clouddn.com/flappybird.gif"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post",offset:12,offset_float:0,b2t:!1,scrollpercent:!1},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="https://ricky.moe/2016/07/19/Keras-DQN-Flappybird/"><title>200行Keras代码实现DQN玩转FlappyBird | 知识沉言</title><script>!function(e,t,a,n,c,o,s){e.GoogleAnalyticsObject=c,e[c]=e[c]||function(){(e[c].q=e[c].q||[]).push(arguments)},e[c].l=1*new Date,o=t.createElement(a),s=t.getElementsByTagName(a)[0],o.async=1,o.src=n,s.parentNode.insertBefore(o,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-41015557-5","auto"),ga("send","pageview")</script></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">知识沉言</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description">at ricky</h1></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="https://ricky.moe/2016/07/19/Keras-DQN-Flappybird/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Ricky"><meta itemprop="description" content=""><meta itemprop="image" content="/uploads/avatar.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="知识沉言"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">200行Keras代码实现DQN玩转FlappyBird</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2016-07-19T09:53:15+08:00">2016-07-19 </time></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/2016/07/19/Keras-DQN-Flappybird/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="2016/07/19/Keras-DQN-Flappybird/" itemprop="commentCount"></span> </a></span><span id="/2016/07/19/Keras-DQN-Flappybird/" class="leancloud_visitors" data-flag-title="200行Keras代码实现DQN玩转FlappyBird"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数 </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>基于Keras的200 行 Python 代码实现 DQN 玩 FlappyBird<br><br><img src="http://7xwkbq.com1.z0.glb.clouddn.com/flappybird.gif" alt="运行图"></p><a id="more"></a><h2 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h2><p>本项目将介绍如何基于Keras使用Deep-Q Learning算法来玩转Flappy Bird<br>这篇文章主要针对是对强化学习感兴趣的初学者。</p><h2 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h2><ul><li>Python 2.7</li><li>Keras 1.0</li><li>pygame</li><li>scikit-image</li></ul><h2 id="如何运行？"><a href="#如何运行？" class="headerlink" title="如何运行？"></a>如何运行？</h2><h3 id="仅CPU-TensorFlow"><a href="#仅CPU-TensorFlow" class="headerlink" title="仅CPU ( TensorFlow )"></a>仅CPU ( TensorFlow )</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/yanpanlau/Keras-FlappyBird.git</div><div class="line">cd Keras-FlappyBird</div><div class="line">python qlearn.py -m &quot;Run&quot;</div></pre></td></tr></table></figure><h3 id="GPU版本-Theano"><a href="#GPU版本-Theano" class="headerlink" title="GPU版本 ( Theano )"></a>GPU版本 ( Theano )</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/yanpanlau/Keras-FlappyBird.git</div><div class="line">cd Keras-FlappyBird</div><div class="line">THEANO_FLAGS=device=gpu,floatX=float32,lib.cnmem=0.2 python qlearn.py -m &quot;Run&quot;</div></pre></td></tr></table></figure><p><code>lib.cnmem</code>=0.2 表示你仅分配20%的显存用来运行程序</p><p><strong>PS:</strong> 如果你想自己训练神经网络，请删除根目录下”model.h5”后运行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">qlearn.py -m &quot;Train&quot;</div></pre></td></tr></table></figure><h2 id="什么是Deep-Q-Network"><a href="#什么是Deep-Q-Network" class="headerlink" title="什么是Deep Q-Network?"></a>什么是Deep Q-Network?</h2><p>Deep Q-NetWork 是Google DeepMind 开发用来玩Atari游戏的学习算法。他们论证了一个计算机如何仅仅通过观察屏幕像素和得分的变化，在“玩”2600次Atari游戏中进行学习。研究成果是令人惊叹的，因为它证实了算法足够聪明应对各种各样的游戏。</p><p>如果你对深度强化学习感兴趣的话，强烈推荐你阅读下面的文章：</p><p><a href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/" target="_blank" rel="external">Demystifying Deep Reinforcement Learning</a></p><h2 id="代码详解"><a href="#代码详解" class="headerlink" title="代码详解"></a>代码详解</h2><p>现在来逐行分析<code>qlearn.py</code>。如果你对Keras和DQN很熟悉的话，你可以跳过这一部分。</p><p>代码要做的事情可以总结如下：</p><ol><li>代码获取游戏屏幕像素数组输入</li><li>代码对图像进行预处理</li><li>处理图像会被喂给卷积神经网络，然后神经网络将决定最佳动作( Flap或者不Flap )</li><li>神经网络使用Q-learning算法训练数百万次</li></ol><h2 id="游戏屏幕输入"><a href="#游戏屏幕输入" class="headerlink" title="游戏屏幕输入"></a>游戏屏幕输入</h2><p>首先，FlappyBird已经包含在Python开发包<code>pygame</code>中了。</p><p>以下是使用FlappyBird API的代码片段:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import wrapped_flappy_bird as game</div><div class="line">x_t1_colored, r_t, terminal = game_state.frame_step(a_t)</div></pre></td></tr></table></figure><p>FlappyBird的API非常简单，函数<code>frame_step</code>的输入<code>a_t</code>只有0和1：</p><ul><li>0 : 表示不flap</li><li>1 : 表示flap</li></ul><p>然后API会返回输出<code>x_t1_colored</code>,<code>r_t</code>和<code>terminal</code>。<br>其中，<code>r_t</code>有三种表示：</p><ul><li>0.1 : 表示bird是活的</li><li>+1 : 表示bird通过了管道</li><li>-1 : 表示bird死掉了</li></ul><p>其中，<code>terminal</code>是一个二值标志表示游戏是否结束。</p><p>DeepMind 建议将得分限定在[-1,+1]区间来改善稳定性。然而我还没有机会去测试不同的得分函数对最终性能表现的影响。感兴趣的读者可以尝试自行修改得分函数：<code>game/wrapped_flapy_bird.py</code>下的<code>def frame_step(self, input_actions)</code>函数</p><h2 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h2><p><img src="/2016/07/19/Keras-DQN-Flappybird/flappybird2.jpg" alt="Flappybird"></p><p>为了让代码训练速度更快，做一些图像预处理的工作非常重要。以下是关键步骤：</p><ol><li>先将彩色图像转换为灰度图</li><li>调整图像的大小为 80 X 80 像素</li><li>一次性提供4帧图像给神经网络进行训练</li></ol><p>为什么要一次性提供4帧图像呢？这是让模型能够理解Bird的速度信息的方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">x_t1 = skimage.color.rgb2gray(x_t1_colored)</div><div class="line">x_t1 = skimage.transform.resize(x_t1,(80,80))</div><div class="line">x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range=(0, 255))</div><div class="line"></div><div class="line">x_t1 = x_t1.reshape(1, 1, x_t1.shape[0], x_t1.shape[1])</div><div class="line">s_t1 = np.append(x_t1, s_t[:, :3, :, :], axis=1)</div></pre></td></tr></table></figure><p><code>x_t1</code>是单独的一帧(1x1x80x80)，而<code>s_t1</code>是多帧图像(1x4x80x80)。你可能会问，为什么输入的维度是(1x4x80x80)而不是(4x80x80)？好吧，这个是Kerasde要求，我们遵守它就好了。</p><p><strong>PS:</strong>一些读者可能会问为什么<code>axis=1</code>?这个的意思是当我在存多帧图像时，我是存储在第二维的，也即(1x<strong>4</strong>x80x80)中。</p><h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>现在，我们能把处理过的屏幕输入到神经网络中了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">def buildmodel():</div><div class="line">    print(&quot;Now we build the model&quot;)</div><div class="line">    model = Sequential()</div><div class="line">    model.add(Convolution2D(32, 8, 8, subsample=(4,4),init=lambda shape, name: normal(shape, scale=0.01, name=name), border_mode=&apos;same&apos;,input_shape=(img_channels,img_rows,img_cols)))</div><div class="line">    model.add(Activation(&apos;relu&apos;))</div><div class="line">    model.add(Convolution2D(64, 4, 4, subsample=(2,2),init=lambda shape, name: normal(shape, scale=0.01, name=name), border_mode=&apos;same&apos;))</div><div class="line">    model.add(Activation(&apos;relu&apos;))</div><div class="line">    model.add(Convolution2D(64, 3, 3, subsample=(1,1),init=lambda shape, name: normal(shape, scale=0.01, name=name), border_mode=&apos;same&apos;))</div><div class="line">    model.add(Activation(&apos;relu&apos;))</div><div class="line">    model.add(Flatten())</div><div class="line">    model.add(Dense(512, init=lambda shape, name: normal(shape, scale=0.01, name=name)))</div><div class="line">    model.add(Activation(&apos;relu&apos;))</div><div class="line">    model.add(Dense(2,init=lambda shape, name: normal(shape, scale=0.01, name=name)))</div><div class="line">   </div><div class="line">    adam = Adam(lr=1e-6)</div><div class="line">    model.compile(loss=&apos;mse&apos;,optimizer=adam)</div><div class="line">    print(&quot;We finish building the model&quot;)</div><div class="line">    return model</div></pre></td></tr></table></figure><p>下面来解析一下上述代码：神经网络的输入包含 4x80x80 的图片。</p><ul><li>第一层: 8x8的32个卷积核，采样步长是4，采用<code>Relu</code>激活函数</li><li>第二层: 4x4的64个卷积核，采样步长是2，采用<code>Relu</code>激活函数</li><li>第三层: 3x3的64个卷积核，采样步长是1，采用<code>Relu</code>激活函数</li></ul><p>最后隐藏层是全连接的，包含512个节点。输出是一个单节点全连接线性层。</p><p>等一下！什么是卷积？理解卷积最好的方式就是想象有一个滑动窗口函数作用在矩阵上。下面的GIF清晰地展现了卷积的过程。</p><p><img src="/2016/07/19/Keras-DQN-Flappybird/convolution_schematic.gif" alt="卷积示意图"></p><p>你可能再会问，进行卷积操作的目的是什么？实际上，卷积作用是帮助计算机理解图像的更高层的特性，比如边缘和轮廓。下面的例子展现了图像在卷积操作后清晰展现出了边缘。</p><p><img src="/2016/07/19/Keras-DQN-Flappybird/edge-detect.jpg" alt="卷积后展现边缘"></p><p>关于更多有关于神经网络的卷积运算的知识，可以参阅<a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/" target="_blank" rel="external">Understanding Convolution Neural Networks for NLP</a></p><h2 id="需要注意的"><a href="#需要注意的" class="headerlink" title="需要注意的"></a>需要注意的</h2><p>Keras让构建卷积神经网络变得十分简单。然而，这里有一些我想要强调的要点：</p><ol><li><p>选取正确的初始化函数非常重要。我选取的正态分布参数<code>σ=0.01</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">init=lambda shape, name: normal(shape, scale=0.01, name=name)</div></pre></td></tr></table></figure></li><li><p>维度的顺序是非常重要的。默认的设置是 4x80x80 (Theano方式)，但如果你的输入是 80x80x4 (Tensorflow方式) ，你就会遇到麻烦，这个时候你需要做的是设置<code>dim_ordering = tf</code> (tf表示tensorflow,th表示theano)</p></li><li>在Keras中，<code>subsample=(2,2)</code>表示把(80x80)大小的图像降采样为(40x40)</li><li>我们采用了自适应学习算法ADAM在进行优化。学习速度为<code>1-e6</code></li></ol><p>对更多其他学习算法感兴趣的朋友可以查阅文章：<a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="external">An overview of gradient descent optimization algorithms</a></p><h2 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h2><p>终于，我们能使用Q-learing算法来训练神经网络了。</p><p>那到底什么是Q-learning呢？要搞清楚什么是Q-learning，首先要弄明白Q-function，也即Q(s,a)。Q(s,a)表示当我们出于state<code>s</code>时选择action<code>a</code>的最大未来得分预估。<code>Q(s,a)</code>会给你一个在state<code>s</code>时选择action<code>a</code>的量化预计。</p><p>你可能会问：</p><ol><li>为什么Q-function有用？</li><li>怎么样才能得到Q-function？</li></ol><p>让我来对Q-function作一个简单的比喻：假设你在玩一个非常难的RPG游戏，而且你又不知道怎么玩。如果说你带了一本攻略，里面包含了所有特定场合下的暗示和指导，那么这样玩游戏是不是很容易呢？你只需要照着攻略做就好。在这里，Q-function就是这样一本攻略书。假设你在state<code>s</code>下，然后你需要决定是选action<code>a</code>还是<code>b</code>。如果你有这个神奇的Q-function，选择就非常简单–选择带有最高Q-value的action即可！</p><p>$$<br>\pi (s)={ argmax }_{ a }Q(s,a)<br>$$</p><p>在这里，π表示policy，注意这个表示会在ML的论文中经常出现。<br>随着时间流动，定义动作的总未来得分为</p><p>$$<br>{ R_{ t }=r_{ t }+r_{ t+1 }+r_{ t+2 }…+r_{ n } }<br>​​$$</p><p>但是因为我们的环境是随机的无法确定，越远时间我们越难估计。因此，定义一个<strong>未来得分折扣（discount future reward）</strong>是比较通用的做法：</p><p>$$<br>R_{ t }=r_{ t }+\gamma r_{ t+1 }+\gamma ^{ 2 }r_{ t+2 }…+\gamma ^{ n-t }r_{ n }<br>$$</p><p>也就是说，未来的得分都会加上一个和折扣（discount）有关的系数$\gamma$。<br>回顾一下Q-function的定义（在state<code>s</code>下选择action<code>a</code>的max future reward）：</p><p>$$<br>Q(s_{ t },a_{ t })=maxR_{ t+1 }<br>$$</p><p>因此，我们能重写Q-function：</p><p>$$<br>Q(s,a)=r+\gamma \ast max_{ a’ }Q(s’,a’)<br>$$</p><p>用通俗的话讲，当前state和action下的<code>(s,a)</code>的<code>max future reward</code>等于当前reward<code>r</code>加上下一个state和action的(${s​​’​,a​​’}$​​​​)的<code>max future reward</code></p><p>我们现在能通过迭代的方法来求解Q-function。给定$(s,a,r,s’)$，我们将这个episode转换为神经网络的训练集，也即，我们试图让$r+γmax​a​​Q(s,a)$等于$Q(s,a)$。你现在能把求解Q-value作为一个回归分析任务。我有一个预估函数$r+γmax​a​​Q(s,a)$和预测函数$Q(s,a)$，然后定义均方差(MSE)，也即损失函数：</p><p>$$<br>L=[r+\gamma max_{ a’ }Q(s’,a’)-Q(s,a)]^{ 2 }<br>$$</p><p>给定$(s,a,r,s’)$，怎样最优化我的Q-function使得它返回最小的均方差损失？方法是，随着L的变小，我们的Q-function就会不断的接近最优值。</p><p>现在，你可能会问，神经网络的作用是什么？现在开始讲一下<strong>Deep Q-learning</strong>怎么来的。如果采用数组表的方式表示的话，$Q(s,a)$会包含上百万条states和actions。DQN的思想是<strong>压缩</strong>Q-table（$Q(s,a)$数组表）后用参数$θ$(<strong>我们成为神经网络权重</strong>)表示。因此，不需要考虑处理巨大的数据表，我们只需要考虑神经网络的权重参数：<br>$$<br>Q(s,a)=f_{ \theta }(s)<br>$$</p><p>其中$f$是我们神经网络，输入为$s$，权重参数为$θ$</p><p>下面是对以上解释的实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">if t &gt; OBSERVE:</div><div class="line">    #sample a minibatch to train on</div><div class="line">    minibatch = random.sample(D, BATCH)</div><div class="line"></div><div class="line">    inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 80, 80, 4</div><div class="line">    targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2</div><div class="line"></div><div class="line">    #Now we do the experience replay</div><div class="line">    for i in range(0, len(minibatch)):</div><div class="line">        state_t = minibatch[i][0]</div><div class="line">        action_t = minibatch[i][1]   #This is action index</div><div class="line">        reward_t = minibatch[i][2]</div><div class="line">        state_t1 = minibatch[i][3]</div><div class="line">        terminal = minibatch[i][4]</div><div class="line">        # if terminated, only equals reward</div><div class="line"></div><div class="line">        inputs[i:i + 1] = state_t    #I saved down s_t</div><div class="line"></div><div class="line">        targets[i] = model.predict(state_t)  # Hitting each buttom probability</div><div class="line">        Q_sa = model.predict(state_t1)</div><div class="line"></div><div class="line">        if terminal:</div><div class="line">            targets[i, action_t] = reward_t</div><div class="line">        else:</div><div class="line">            targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)</div><div class="line"></div><div class="line">        loss += model.train_on_batch(inputs, targets)</div><div class="line"></div><div class="line">    s_t = s_t1</div><div class="line">    t = t + 1</div></pre></td></tr></table></figure><h2 id="重放（Experience-Replay）"><a href="#重放（Experience-Replay）" class="headerlink" title="重放（Experience Replay）"></a>重放（Experience Replay）</h2><p>如果你详细查看以上的代码，你会看到一条“重放（Experience Replay）”的注释。让我来解释一下这个是做什么的：使用非线性函数如神经网络来近似Q-value已经被发现是不稳定的。因此，我的处理方法是，在游行进行中，所有的episode$(s,a,r,s’)$都会存储在<strong>重放内存D</strong>中（我使用Python的<code>deque</code>来存储）。在训练神经网络时，随机的<strong>重放内存D</strong>的<code>mini-batches</code>会被使用，而不是最近邻的episode，这能大大提高神经网络的稳定性。</p><h2 id="广度还是深度？"><a href="#广度还是深度？" class="headerlink" title="广度还是深度？"></a>广度还是深度？</h2><p>这是强化学习算法中的一个经典问题，广度优先还是深度优先？或者说，强化学习需要花多少时间去进行广度搜索，多少时间去进行深度搜索？我们其实在现实生活中也经常遇到类似的情况。比方说，在星期六的晚上，选择去吃哪一家餐厅。我们总是有一个可以选择的餐厅列表，就像有一本<code>Q(s,a)</code>的攻略书一样。如果我们单凭我们平时的一贯口味来看，很大可能我们会从我们知道的餐厅里面选择一家最好的。然而，有时候，我们也会愿意去尝试一些新餐厅，也许会有更好的不是嘛？强化学习也是类似的。为了最大化<code>future reward</code>，强化学习需要平衡现有的policy搜索（也可以称作“贪心”）和新policy搜索的时间开销。一个流行的方式叫做${\partial}$贪心策略。${\partial}$的取值范围是<code>[0,1]</code>，决定了强化学习会花多长比例的时间随机地开始广度搜索。这个方法帮助算法不时的尝试一些新的路径以验证是否还有更好的策略。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">if random.random() &lt;= epsilon:</div><div class="line">    print(&quot;----------Random Action----------&quot;)</div><div class="line">    action_index = random.randrange(ACTIONS)</div><div class="line">    a_t[action_index] = 1</div><div class="line">        else:</div><div class="line">    q = model.predict(s_t)       #input a stack of 4 images, get the prediction</div><div class="line">    max_Q = np.argmax(q)</div><div class="line">    action_index = max_Q</div><div class="line">    a_t[max_Q] = 1</div></pre></td></tr></table></figure><p>希望本篇文章能帮助你理解DQN是如何工作的。</p><h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><h3 id="我的训练速度非常慢"><a href="#我的训练速度非常慢" class="headerlink" title="我的训练速度非常慢"></a>我的训练速度非常慢</h3><p>可能你需要一个GPU来加速计算。我使用的是TITAN X，训练了至少1百万帧才得到一个差不多的结果。</p><h2 id="后续的工作和思考"><a href="#后续的工作和思考" class="headerlink" title="后续的工作和思考"></a>后续的工作和思考</h2><ol><li>现在的DQN依赖于大量的重放（Experience Replay）。是否有可能替换或者去掉这个步骤？</li><li>如果决定最优化卷积神经网络的方法？</li><li>训练速度很慢，有没有加速的方法？</li><li>神经网络究竟学了什么？这个模型是否可以迁移？</li></ol><p>我相信这些问题还没有得到很好的解决。这也是Matching Learning正在活跃研究的部分。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] Mnih Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level Control through Deep Reinforcement Learning. Nature, 529-33, 2015.</p><h2 id="声明"><a href="#声明" class="headerlink" title="声明"></a>声明</h2><p>本文章的工作高度依赖于下面的repo：</p><ol><li><a href="https://github.com/yenchenlin/DeepLearningFlappyBird" target="_blank" rel="external">https://github.com/yenchenlin/DeepLearningFlappyBird</a></li><li><a href="http://edersantana.github.io/articles/keras_rl/" target="_blank" rel="external">http://edersantana.github.io/articles/keras_rl/</a></li></ol><h2 id="译者声明"><a href="#译者声明" class="headerlink" title="译者声明"></a>译者声明</h2><p>本文章来自于<a href="https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html" target="_blank" rel="external">Using Keras and Deep Q-Network to Play FlappyBird</a></p><p>修改了原文中一些明显错误。<br>向原作者表示致敬。</p></div><div></div><div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Keras/" rel="tag"># Keras</a> <a href="/tags/DQN/" rel="tag"># DQN</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2016/07/19/Keras-Vgg16/" rel="prev" title="Keras-Vgg16">Keras-Vgg16 <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"></div></div></div><div class="comments" id="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview">站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/uploads/avatar.jpg" alt="Ricky"><p class="site-author-name" itemprop="name">Ricky</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">16</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">38</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/RickyWong33" target="_blank" title="Github"><i class="fa fa-fw fa-globe"></i> Github </a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/rickywong/" target="_blank" title="zhihu"><i class="fa fa-fw fa-globe"></i> zhihu</a></span></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#概要"><span class="nav-number">1.</span> <span class="nav-text">概要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#安装依赖"><span class="nav-number">2.</span> <span class="nav-text">安装依赖</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#如何运行？"><span class="nav-number">3.</span> <span class="nav-text">如何运行？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#仅CPU-TensorFlow"><span class="nav-number">3.1.</span> <span class="nav-text">仅CPU ( TensorFlow )</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GPU版本-Theano"><span class="nav-number">3.2.</span> <span class="nav-text">GPU版本 ( Theano )</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#什么是Deep-Q-Network"><span class="nav-number">4.</span> <span class="nav-text">什么是Deep Q-Network?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#代码详解"><span class="nav-number">5.</span> <span class="nav-text">代码详解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#游戏屏幕输入"><span class="nav-number">6.</span> <span class="nav-text">游戏屏幕输入</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#图像处理"><span class="nav-number">7.</span> <span class="nav-text">图像处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积神经网络"><span class="nav-number">8.</span> <span class="nav-text">卷积神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#需要注意的"><span class="nav-number">9.</span> <span class="nav-text">需要注意的</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DQN"><span class="nav-number">10.</span> <span class="nav-text">DQN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#重放（Experience-Replay）"><span class="nav-number">11.</span> <span class="nav-text">重放（Experience Replay）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#广度还是深度？"><span class="nav-number">12.</span> <span class="nav-text">广度还是深度？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FAQ"><span class="nav-number">13.</span> <span class="nav-text">FAQ</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#我的训练速度非常慢"><span class="nav-number">13.1.</span> <span class="nav-text">我的训练速度非常慢</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#后续的工作和思考"><span class="nav-number">14.</span> <span class="nav-text">后续的工作和思考</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">15.</span> <span class="nav-text">参考</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#声明"><span class="nav-number">16.</span> <span class="nav-text">声明</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#译者声明"><span class="nav-number">17.</span> <span class="nav-text">译者声明</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Ricky</span></div><div class="powered-by">由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动</div><div class="theme-info">主题 - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv">本站访客数 <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> 人次 </span><span class="site-pv">本站总访问量 <span class="busuanzi-value" id="busuanzi_value_site_pv"></span> 次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script><script id="dsq-count-scr" src="https://ricky-moe.disqus.com/count.js" async></script><script type="text/javascript">var disqus_config=function(){this.page.url="https://ricky.moe/2016/07/19/Keras-DQN-Flappybird/",this.page.identifier="2016/07/19/Keras-DQN-Flappybird/",this.page.title="200行Keras代码实现DQN玩转FlappyBird"},d=document,s=d.createElement("script");s.src="https://ricky-moe.disqus.com/embed.js",s.setAttribute("data-timestamp",""+ +new Date),(d.head||d.body).appendChild(s)</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("ajqvGX9WwzOqG9cWfeXxjWXX-gzGzoHsz","EbbP1N1QlwfolxtLR7IbDGvK")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){var t=".leancloud-visitors-count";if(0===e.length)return void o.find(t).text(0);for(var i=0;i<e.length;i++){var r=e[i],s=r.get("url"),l=r.get("time"),c=document.getElementById(s);$(c).find(t).text(l)}for(var i=0;i<n.length;i++){var s=n[i],c=document.getElementById(s),u=$(c).find(t);""==u.text()&&u.text(0)}}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var r=new e,s=new AV.ACL;s.setPublicReadAccess(!0),s.setPublicWriteAccess(!0),r.setACL(s),r.set("title",o),r.set("url",n),r.set("time",1),r.save(null,{success:function(e){var t=$(document.getElementById(n));t.find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html><!-- rebuild by neat -->